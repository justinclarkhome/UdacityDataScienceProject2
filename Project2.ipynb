{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "sys.path.append('./data')\n",
    "sys.path.append('./models')\n",
    "\n",
    "# from train_classifier import load_data\n",
    "\n",
    "# Downloads for NLTK tools\n",
    "nltk.download('words', quiet=True);\n",
    "nltk.download('wordnet', quiet=True);\n",
    "nltk.download('punkt_tab', quiet=True);\n",
    "nltk.download('stopwords', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "## ETL Pipeline: process_data.py\n",
    "In a Python script, process_data.py, write a data cleaning pipeline that:\n",
    "- Loads the messages and categories datasets\n",
    "- Merges the two datasets\n",
    "- Cleans the data\n",
    "- Stores it in a SQLite database\n",
    "\n",
    "## ML pipeline: train_classifier.py\n",
    "In a Python script, train_classifier.py, write a machine learning pipeline that:\n",
    "- Loads data from the SQLite database\n",
    "- Splits the dataset into training and test sets\n",
    "- Builds a text processing and machine learning pipeline\n",
    "- Trains and tunes a model using GridSearchCV\n",
    "- Outputs results on the test set\n",
    "- Exports the final model as a pickle file\n",
    "\n",
    "## Flask Web App\n",
    "We are providing much of the flask web app for you, but feel free to add extra features depending on your knowledge of flask, html, css and javascript. For this part, you'll need to:\n",
    "- Modify file paths for database and model as needed\n",
    "- Add data visualizations using Plotly in the web app. One example is provided for you.\n",
    "\n",
    "## Github and Code Quality\n",
    "Your project will also be graded based on the following:\n",
    "- Use of Git and Github\n",
    "- Strong documentation\n",
    "- Clean and modular code\n",
    "- Follow the [RUBRIC](https://learn.udacity.com/nanodegrees/nd025/parts/cd0018/lessons/e692c8ed-b713-464b-95ac-72d93a35b4fc/concepts/e692c8ed-b713-464b-95ac-72d93a35b4fc-project-rubric) when you work on your project to assure you meet all of the necessary criteria for developing the pipelines and web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "database_filepath = os.path.join(DATA_DIR, 'DisasterResponse.db')\n",
    "messages_filepath = os.path.join(DATA_DIR, 'disaster_messages.csv')\n",
    "categories_filepath = os.path.join(DATA_DIR, 'disaster_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug process_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import clean_data, load_data, save_data, load_categories_data, load_messages_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = load_messages_data(messages_filepath)\n",
    "# c = load_categories_data(categories_filepath)\n",
    "# df = m.merge(c, on='id')\n",
    "# df = pd.get_dummies(df, columns=['related', 'genre'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(load_data(messages_filepath, categories_filepath))\n",
    "save_data(df, database_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug train_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_classifier import load_data, display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "database_filepath = os.path.join(DATA_DIR, 'DisasterResponse.db')\n",
    "X, Y, category_names = load_data(database_filepath)\n",
    "\n",
    "# create dummies for genre and related - we can \"recover\" them later\n",
    "Y_dummies = ['related']\n",
    "X_dummies = ['genre']\n",
    "Y = pd.get_dummies(Y, columns=Y_dummies, dtype=int)\n",
    "X = pd.get_dummies(X, columns=X_dummies, dtype=int)\n",
    "\n",
    "X = X[['message']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=2/3, test_size=1/3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_dummies(df, dummies_in_df):\n",
    "    dummy_columns = []\n",
    "    for dummy in dummies_in_df:\n",
    "        dummy_columns += [i for i in df.columns if f'{dummy}_' in i]\n",
    "        df = pd.concat([\n",
    "            df.drop(dummy_columns, axis=1),\n",
    "            pd.from_dummies(df[dummy_columns], sep='_'),\n",
    "        ],axis=1)\n",
    "    return df\n",
    "\n",
    "# X = recover_dummies(X, X_dummies)\n",
    "# Y = recover_dummies(Y, Y_dummies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script uses a custom tokenize function using nltk to case normalize, lemmatize, and tokenize text. This function is used in the machine learning pipeline to vectorize and then apply TF-IDF to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lemmatize=True, make_lowercase=True, remove_non_words=True, remove_stop_words=True, verbose=False):\n",
    "    def get_tokens_per_row(text, make_lowercase):\n",
    "        if verbose:\n",
    "            print('... tokenizing and converting to lowercase.' if make_lowercase else '... tokenizing.')\n",
    "        return [nltk.word_tokenize(s.lower() if make_lowercase else s) for s in text]\n",
    "    \n",
    "    def _remove_non_words(tokens_by_message):\n",
    "        if verbose:\n",
    "            print('... removing non-words.')\n",
    "        answer = []\n",
    "        for tokens in tokens_by_message:\n",
    "            answer.append([i for i in tokens if i.isalpha()])\n",
    "        return answer\n",
    "\n",
    "    def _remove_stop_words(tokens_by_message, stop_words=set(nltk.corpus.stopwords.words('english'))):\n",
    "        if verbose:\n",
    "            print('... removing stop words.')\n",
    "        answer = []\n",
    "        for tokens in tokens_by_message:\n",
    "            answer.append([i for  i in tokens if i not in stop_words])\n",
    "        return answer\n",
    "    \n",
    "    def _lemmatize(tokens_by_message, lemmatizer=nltk.stem.WordNetLemmatizer()):\n",
    "        if verbose:\n",
    "            print('... lemmatizing.')\n",
    "        answer = []\n",
    "        for tokens in tokens_by_message:\n",
    "            answer.append([lemmatizer.lemmatize(i).strip() for i in tokens])\n",
    "        return answer\n",
    "\n",
    "    tokens_by_message = get_tokens_per_row(text, make_lowercase)\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens_by_message = _lemmatize(tokens_by_message)\n",
    "\n",
    "    if remove_stop_words:\n",
    "        tokens_by_message = _remove_stop_words(tokens_by_message)\n",
    "        \n",
    "    if remove_non_words:\n",
    "        tokens_by_message = _remove_non_words(tokens_by_message)\n",
    "\n",
    "    # now combine back to strings (one per row) so we can pass it back to TfidfTransformer\n",
    "    processed_messages = [' '.join(tokens) for tokens in tokens_by_message]\n",
    "\n",
    "    return processed_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = X.message.iloc[0:50]\n",
    "# processed_messages = tokenize(text)\n",
    "# vectorizer = TfidfVectorizer() \n",
    "# tfidf = vectorizer.fit_transform(processed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, column_to_process='message'):\n",
    "#         self.column_to_process = column_to_process\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "#     def transform(self, X, y=None):\n",
    "#         return tokenize(X[self.column_to_process])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you: https://stackoverflow.com/questions/67768470/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-6-80\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('vect', TfidfVectorizer(tokenizer=tokenize), 'message')\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', transformer),\n",
    "    ('model', RandomForestClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM Flow\n",
    "- ALL of this should go in the README.md file at the end of the project.\n",
    "- The rubric states the README should contain a lot of info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "- The purpose of this project is to identify an efficient way to model emergency messages to determine which are most informative for making a quick disaster response decision. This is important as resources for disaster response are limited and false positives can prevent those resources from being deployed where they are needed most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_columns = [k for k, v in df.dtypes.items() if v in [int, np.int64]]\n",
    "# df[[k for k, v in df.dtypes.items() if v in [int, np.int64]]].describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "- The dataset is raw, containing an identifier along with communication information that can contain an English 'message', a non-English 'original' messge, and a 'genre' (type of communication). This insformation is provided in CSV form.\n",
    "- The content of each row is inconsistent - for example, some of the lines contain more than 4 commas (which would cleanly correspond to the 4 expected fields), some contain consecutive commas (where one of the messages is missing), and some contain quotation marks separating the English and non-English messages.\n",
    "- There is also categorical information stored in a separate CSV, which provides some informative identifiers that are pre-associated with each message (and link by an 'id' field in each set of data).\n",
    "    - This CSV data is further subdivided into key/value pairs separated by a semicolon, e.g. 'id,key1-value1;key2-value2; ...' etc.\n",
    "    - There are 36 categeries in the file, and the rubric says these are to be used as the ***responses*** for the multi-output classification task.\n",
    "    - Note that the values in the category values are integers - mostly boolean (0 or 1) except for **related** which is ternery (0/1/2).\n",
    "        - If using a decision tree style classifier, we do not need to create dummies for the **related** variable.\n",
    "    - These categories are also NOT mutually exclusive (the cross-sectional sum of them can be greater than 1).\n",
    "\n",
    "- **load_data.py** loads the raw message and category information, first splits out the 'id' and the 'categories' key/value pairs, then splits the key/value pairs into columns of data with integer data, and joins each dataset (on 'id') and passes that joined dataframe along for cleaning in the next phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- **clean_data.py** attempts to separate the Enlish and non-English components of the raw nessage, and stores the processed English-only message as a new field in the data, allowing it to be more easily processed in the machine learning pipeline.\n",
    "- It also converts the 'genre' field into a set of dummies with boolean values (n-1 categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
